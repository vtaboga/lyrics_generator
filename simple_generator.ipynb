{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "\n",
    "import torch.utils.data as data\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "np.random.seed(1919788)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for padding purpuses, with have been once through out the  all corpus to find the longest sentence :\n",
    "max_sentence_length = 114\n",
    "\n",
    "#load dictionnaries :\n",
    "char_to_ix = np.load('char_to_ix.npy').item()\n",
    "ix_to_char = np.load('ix_to_char.npy').item()\n",
    "chars = np.load('chars.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_encoder(song, char_to_ix):\n",
    "    \"\"\"change str characters with matching encode number\"\"\"\n",
    "    encoded = [char_to_ix[char] for _,char in enumerate(song)]\n",
    "\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_path, artists = None) :\n",
    "    \"\"\"read csv file with pandas\n",
    "    keep specific songs of an artist\n",
    "    return df and number of songs\"\"\"\n",
    "    \n",
    "    data = pd.read_csv(\"songdata.csv\")\n",
    "    if artists :\n",
    "        data = data[data.artist.isin(artists)].reset_index()\n",
    "    n_songs = len(data)\n",
    "    return data, n_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_lines(song) :\n",
    "    \"\"\"separate lines of a song\n",
    "    input : lyrics (str)\n",
    "    output : list of every lines\"\"\"\n",
    "    \n",
    "    song_lyrics = []\n",
    "    line = \"\"\n",
    "\n",
    "    for i in range(len(song)):\n",
    "        #add the caracter to the line\n",
    "        line+=song[i]\n",
    "        #check two following caracters to spot the \\\\n\n",
    "        if song[i]==\"\\n\" :\n",
    "            #cut the str to end the line\n",
    "            song_lyrics.append(line[:len(line)-2])\n",
    "            #start another line\n",
    "            line =''\n",
    "            \n",
    "    return song_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(encoded_text, sequence_length):\n",
    "    sequences = []\n",
    "    n_sequences = int(len(encoded_text)/sequence_length)\n",
    "    for i in range(n_sequences):\n",
    "        sequences.append(encoded_text[i*sequence_length:(i+1)*sequence_length])\n",
    "    return(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(seq, max_length, pad_label=100):\n",
    "    \n",
    "    seq += [pad_label for i in range(max_length - len(seq))]\n",
    "    \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_str(n_songs, df):\n",
    "    \"\"\"concatenate lyrics from the n_songs of a df\"\"\"\n",
    "    songs_conc = \"\"\n",
    "    for i in range(n_songs):\n",
    "        songs_conc += df.iloc[i].text\n",
    "    return songs_conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeds_array(array, n_chars, n_sequences, max_sequence_length):\n",
    "    \"\"\"inputs : 2 dim array of padded encoded sequences and number of char\n",
    "    outputs : 3 dim array, replace each encoded char by a one hot vector\"\"\"\n",
    "    \n",
    "    #create an empty array of the rigth dimension\n",
    "    output = np.zeros((n_sequences, max_sequence_length, n_chars+1))\n",
    "\n",
    "    for seq in range(n_sequences):\n",
    "        for char in range(max_sequence_length):\n",
    "            label = array[seq][char]\n",
    "            #100 = pad label\n",
    "            if label != 100 : \n",
    "                output[seq][char][label] = 1\n",
    "            else :\n",
    "                #replace the first number by 1 (pad label)\n",
    "                #char_to_ix has been made to start at 1 for the first character\n",
    "                output[seq][char][0] = 1\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeds_array_wp(array, n_chars, n_sequences, max_sequence_length):\n",
    "    \"\"\"inputs : 2 dim array of padded encoded sequences and number of char\n",
    "    outputs : 3 dim array, replace each encoded char by a one hot vector\"\"\"\n",
    "    \n",
    "    #create an empty array of the rigth dimension\n",
    "    output = np.zeros((n_sequences, max_sequence_length, n_chars))\n",
    "\n",
    "    for seq in range(n_sequences):\n",
    "        for char in range(max_sequence_length):\n",
    "            label = array[seq][char] \n",
    "            output[seq][char][label] = 1\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shaping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df\n",
    "df, n_songs = load_data(\"songdata.csv\",['Elton John'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#gather songs in one big str\n",
    "songs_conc = concat_str(n_songs, df)\n",
    "#create a list of the lines of our dataset\n",
    "sequences = split_lines(songs_conc)\n",
    "#change char for matching char to ix index\n",
    "encoded_sequences = [seq_encoder(sequences[i], char_to_ix) for i in range(len(sequences))]\n",
    "#remove empty sequences\n",
    "encoded_sequences = [encoded_sequences[i] for i in range(len(encoded_sequences)) if len(encoded_sequences[i])>1]\n",
    "#pad sequences according to the longest setence in the all df \n",
    "#in this way the nn dimensions are independent of the chosen artist\n",
    "encoded_padded_sequences = [pad_sequence(encoded_sequences[i], max_sentence_length) for i in range(len(encoded_sequences))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 50\n",
    "#gather songs in one big str\n",
    "songs_conc = concat_str(n_songs,df)\n",
    "#change char for matching char_to_ix index\n",
    "encoded_sequences = seq_encoder(songs_conc, char_to_ix)\n",
    "#split text in size learnable sequences\n",
    "encoded_split_sequences = split_sequences(encoded_sequences, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sequences = len(encoded_split_sequences)\n",
    "\n",
    "#slice inputs and outputs by one char\n",
    "inputs = [encoded_split_sequences[i][:-1] for i in range(n_sequences)]\n",
    "outputs = [encoded_split_sequences[i][1:] for i in range(n_sequences)]\n",
    "        \n",
    "#embeds the inputs and outputs\n",
    "inputs_embedded = embeds_array_wp(array = inputs, n_chars = len(char_to_ix), \n",
    "                               n_sequences = n_sequences, \n",
    "                               max_sequence_length = sequence_length-1)\n",
    "\n",
    "outputs_embedded = embeds_array_wp(array = outputs, n_chars = len(char_to_ix), \n",
    "                               n_sequences = n_sequences, \n",
    "                               max_sequence_length = sequence_length-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_inputs = inputs_embedded[:3000]\n",
    "lit_outputs = outputs_embedded[:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters :\n",
    "hidden_dim = 514\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.LSTM(hidden_dim, input_shape=(None, len(char_to_ix)), return_sequences=True))\n",
    "for i in range(num_layers - 1):\n",
    "    model.add(keras.layers.LSTM(num_layers, return_sequences=True))\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Dense(len(char_to_ix))))\n",
    "model.add(keras.layers.Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, length, vocab_size):\n",
    "    \n",
    "    ix = [np.random.randint(1,vocab_size)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    \n",
    "    for i in range(length):\n",
    "        #fill the one hot vector of the corresponding caracter\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    return y_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** epoch 0 *****\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taboga/virtualenvs/env1/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.5641\n",
      "***** epoch 1 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.5536\n",
      "***** epoch 2 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.5435\n",
      "***** epoch 3 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.5336\n",
      "***** epoch 4 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.5237\n",
      "***** epoch 5 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.5142\n",
      "***** epoch 6 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.5049\n",
      "***** epoch 7 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.4957\n",
      "***** epoch 8 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.4864\n",
      "***** epoch 9 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.4774\n",
      "***** epoch 10 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.4686\n",
      "***** epoch 11 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.4599\n",
      "***** epoch 12 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.4512\n",
      "***** epoch 13 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.4429\n",
      "***** epoch 14 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.4345\n",
      "***** epoch 15 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.4262\n",
      "***** epoch 16 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.4182\n",
      "***** epoch 17 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.4104\n",
      "***** epoch 18 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.4026\n",
      "***** epoch 19 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.3951\n",
      "***** epoch 20 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.3875\n",
      "***** epoch 21 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.3800\n",
      "***** epoch 22 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.3730\n",
      "***** epoch 23 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.3659\n",
      "***** epoch 24 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.3589\n",
      "***** epoch 25 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.3520\n",
      "***** epoch 26 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.3452\n",
      "***** epoch 27 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.3385\n",
      "***** epoch 28 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.3320\n",
      "***** epoch 29 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.3255\n",
      "***** epoch 30 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.3192\n",
      "***** epoch 31 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.3131\n",
      "***** epoch 32 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.3068\n",
      "***** epoch 33 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.3009\n",
      "***** epoch 34 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.2948\n",
      "***** epoch 35 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.2889\n",
      "***** epoch 36 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.2830\n",
      "***** epoch 37 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.2772\n",
      "***** epoch 38 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.2717\n",
      "***** epoch 39 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.2661\n",
      "***** epoch 40 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.2607\n",
      "***** epoch 41 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.2553\n",
      "***** epoch 42 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.2501\n",
      "***** epoch 43 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.2450\n",
      "***** epoch 44 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.2397\n",
      "***** epoch 45 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.2347\n",
      "***** epoch 46 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.2298\n",
      "***** epoch 47 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.2249\n",
      "***** epoch 48 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.2201\n",
      "***** epoch 49 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.2152\n",
      "***** epoch 50 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.2105\n",
      "***** epoch 51 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.2059\n",
      "***** epoch 52 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.2014\n",
      "***** epoch 53 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.1968\n",
      "***** epoch 54 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.1926\n",
      "***** epoch 55 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.1883\n",
      "***** epoch 56 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.1840\n",
      "***** epoch 57 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.1799\n",
      "***** epoch 58 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.1757\n",
      "***** epoch 59 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.1717\n",
      "***** epoch 60 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.1678\n",
      "***** epoch 61 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.1638\n",
      "***** epoch 62 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.1598\n",
      "***** epoch 63 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.1561\n",
      "***** epoch 64 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.1523\n",
      "***** epoch 65 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.1488\n",
      "***** epoch 66 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.1453\n",
      "***** epoch 67 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.1418\n",
      "***** epoch 68 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.1384\n",
      "***** epoch 69 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.1351\n",
      "***** epoch 70 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.1318\n",
      "***** epoch 71 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.1286\n",
      "***** epoch 72 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.1255\n",
      "***** epoch 73 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.1225\n",
      "***** epoch 74 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.1196\n",
      "***** epoch 75 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.1167\n",
      "***** epoch 76 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.1139\n",
      "***** epoch 77 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.1111\n",
      "***** epoch 78 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.1085\n",
      "***** epoch 79 *****\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.1058\n",
      "***** epoch 80 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.1032\n",
      "***** epoch 81 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.1009\n",
      "***** epoch 82 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0984\n",
      "***** epoch 83 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0959\n",
      "***** epoch 84 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0936\n",
      "***** epoch 85 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0915\n",
      "***** epoch 86 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0891\n",
      "***** epoch 87 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0869\n",
      "***** epoch 88 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0849\n",
      "***** epoch 89 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0828\n",
      "***** epoch 90 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0807\n",
      "***** epoch 91 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0788\n",
      "***** epoch 92 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0769\n",
      "***** epoch 93 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0751\n",
      "***** epoch 94 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0732\n",
      "***** epoch 95 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0715\n",
      "***** epoch 96 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0698\n",
      "***** epoch 97 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0682\n",
      "***** epoch 98 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0665\n",
      "***** epoch 99 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0650\n",
      "***** epoch 100 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0634\n",
      "***** epoch 101 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0622\n",
      "***** epoch 102 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0608\n",
      "***** epoch 103 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0595\n",
      "***** epoch 104 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0582\n",
      "***** epoch 105 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0569\n",
      "***** epoch 106 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0558\n",
      "***** epoch 107 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0546\n",
      "***** epoch 108 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0535\n",
      "***** epoch 109 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0524\n",
      "***** epoch 110 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0514\n",
      "***** epoch 111 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0504\n",
      "***** epoch 112 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0495\n",
      "***** epoch 113 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0485\n",
      "***** epoch 114 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0476\n",
      "***** epoch 115 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0468\n",
      "***** epoch 116 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0460\n",
      "***** epoch 117 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0452\n",
      "***** epoch 118 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0444\n",
      "***** epoch 119 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0437\n",
      "***** epoch 120 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0430\n",
      "***** epoch 121 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0423\n",
      "***** epoch 122 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0416\n",
      "***** epoch 123 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0410\n",
      "***** epoch 124 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0404\n",
      "***** epoch 125 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0397\n",
      "***** epoch 126 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0391\n",
      "***** epoch 127 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0385\n",
      "***** epoch 128 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0380\n",
      "***** epoch 129 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0375\n",
      "***** epoch 130 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0370\n",
      "***** epoch 131 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0364\n",
      "***** epoch 132 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0359\n",
      "***** epoch 133 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0353\n",
      "***** epoch 134 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0348\n",
      "***** epoch 135 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0343\n",
      "***** epoch 136 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0338\n",
      "***** epoch 137 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0333\n",
      "***** epoch 138 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0329\n",
      "***** epoch 139 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0324\n",
      "***** epoch 140 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0320\n",
      "***** epoch 141 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0316\n",
      "***** epoch 142 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0312\n",
      "***** epoch 143 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0308\n",
      "***** epoch 144 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0304\n",
      "***** epoch 145 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0300\n",
      "***** epoch 146 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0296\n",
      "***** epoch 147 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0292\n",
      "***** epoch 148 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0289\n",
      "***** epoch 149 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0285\n",
      "***** epoch 150 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0282\n",
      "***** epoch 151 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0279\n",
      "***** epoch 152 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0276\n",
      "***** epoch 153 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0272\n",
      "***** epoch 154 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0270\n",
      "***** epoch 155 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0267\n",
      "***** epoch 156 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0263\n",
      "***** epoch 157 *****\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0260\n",
      "***** epoch 158 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0257\n",
      "***** epoch 159 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0255\n",
      "***** epoch 160 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0252\n",
      "***** epoch 161 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0249\n",
      "***** epoch 162 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0246\n",
      "***** epoch 163 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0244\n",
      "***** epoch 164 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0241\n",
      "***** epoch 165 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0239\n",
      "***** epoch 166 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0237\n",
      "***** epoch 167 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0234\n",
      "***** epoch 168 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0232\n",
      "***** epoch 169 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0230\n",
      "***** epoch 170 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0228\n",
      "***** epoch 171 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0226\n",
      "***** epoch 172 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0224\n",
      "***** epoch 173 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0221\n",
      "***** epoch 174 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0219\n",
      "***** epoch 175 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0217\n",
      "***** epoch 176 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0216\n",
      "***** epoch 177 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0212\n",
      "***** epoch 178 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0212\n",
      "***** epoch 179 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0210\n",
      "***** epoch 180 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0209\n",
      "***** epoch 181 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0207\n",
      "***** epoch 182 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0205\n",
      "***** epoch 183 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0203\n",
      "***** epoch 184 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0202\n",
      "***** epoch 185 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0200\n",
      "***** epoch 186 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0198\n",
      "***** epoch 187 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0197\n",
      "***** epoch 188 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0196\n",
      "***** epoch 189 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0194\n",
      "***** epoch 190 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0193\n",
      "***** epoch 191 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 14s 4ms/step - loss: 3.0191\n",
      "***** epoch 192 *****\n",
      "Epoch 1/1\n",
      "3562/3562 [==============================] - 15s 4ms/step - loss: 3.0190\n",
      "***** epoch 193 *****\n",
      "Epoch 1/1\n",
      "1780/3562 [=============>................] - ETA: 7s - loss: 3.0206 "
     ]
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('***** epoch '+str(epoch)+\" *****\")\n",
    "    model.fit(inputs_embedded, outputs_embedded, batch_size=int(n_sequences/4), verbose=1, nb_epoch=1)\n",
    "    model.save_weights('checkpoint_{}_epoch_{}.hdf5'.format(hidden_dim, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L                                                 "
     ]
    }
   ],
   "source": [
    "new_song = generate_text(model, 50, len(char_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
